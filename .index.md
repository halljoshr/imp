# Project Index

Generated by `imp init` on 2026-02-15.

## Modules
| Module | Purpose | Files | Functions | Classes |
|--------|---------|-------|-----------|---------|
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp | I'll explore the module to understand its purpose and responsibilities.Based on my exploration of the module, here's the business summary:

---

**The `imp` module is an AI-powered engineering workflow framework that automates and enhances software development processes from requirements gathering through code delivery.** It conducts structured requirements interviews using story-based questioning to produce implementation-ready specifications, automatically generates project management tickets, performs AI-driven code reviews, and runs validation checks on projects. The framework tracks token usage and costs across all AI operations and integrates with project management tools like Plane to provide end-to-end visibility and automation of the software development lifecycle. | 1 | 0 | 0 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/cli | The `/imp/cli` module serves as the command-line interface for the Imp framework, an AI-powered engineering workflow system that helps development teams manage their entire software delivery lifecycle. It provides developers with tools to initialize project indexes, validate code quality with automated checks, conduct AI-driven code reviews, and transform requirements specifications into actionable project management tickets. The CLI acts as the primary user touchpoint for leveraging AI automation across common engineering tasks like requirements gathering, planning, validation, and code quality assurance. | 2 | 12 | 1 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/context | The `imp.context` module automatically analyzes software projects to understand their structure and contents. It scans the codebase to discover source files, parses them to extract key information like functions, classes, and dependencies, then generates human-readable index files (`.index.md`) that document the project's organization. This enables developers and AI assistants to quickly comprehend what code exists, how it's organized, and what components are available without manually reviewing every file. | 6 | 17 | 8 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/executor | I'll read the files in the executor module to understand its purpose and responsibilities.Based on the module documentation and usage patterns in the codebase, here's a business-level summary:

**Executor Module Summary:**

The executor module provides a managed, controlled environment for safely running AI-generated code within the Imp engineering workflow framework. It wraps code execution with monitoring and safety controls, ensuring that automated code changes can be executed reliably while tracking resource usage (tokens, costs) consistently with other AI operations. This enables the framework to automate coding tasks while maintaining oversight and preventing uncontrolled or unsafe code execution. | 1 | 0 | 0 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/interview | Based on the module structure and documentation, here's a business summary:

**The `interview` module provides a standardized framework for capturing and validating product requirements from stakeholder conversations.** It defines a strict schema (contract) for requirements specifications that downstream processes depend on—including PM ticket generation, review criteria, and coding agent specifications—while allowing flexibility in how those requirements are gathered through pluggable interview methods. The module includes validation logic that scores specification completeness (0-100) across weighted criteria like component definitions, success metrics, edge cases, and constraints, ensuring requirements are sufficiently detailed before teams invest in implementation work.

In essence, it's a quality gate that prevents incomplete or ambiguous requirements from entering the development pipeline while maintaining a consistent data format regardless of the interview approach used. | 4 | 6 | 8 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/interview/skills | This module serves as a **resource locator for interview content and domain expertise**. It provides standardized access to portable interview skills (templates/frameworks for conducting interviews) and domain-specific knowledge libraries (e.g., software requirements, and other specialized areas). Essentially, it acts as a centralized utility that helps the interview system find and load the appropriate interview methodologies and domain knowledge needed to conduct effective, specialized interviews across different subject areas. | 1 | 3 | 0 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/metrics | The **metrics** module provides comprehensive tracking and monitoring of AI agent resource consumption and performance. It records detailed metrics events for each agent invocation—including token usage, API costs, and execution timing—and stores them persistently for analysis. This enables the system to monitor operational expenses, optimize agent efficiency, and provide visibility into resource utilization across agent workflows. | 4 | 0 | 5 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/pm | Based on the code structure and documentation, here's a business summary of the `imp.pm` module:

**The PM (Project Management) module automates the creation of project management tickets from interview specifications.** It reads interview requirements and automatically generates structured tickets in PM systems like Plane or Linear, creating parent epic tickets and breaking down components into individual tasks with appropriate priorities and descriptions. This bridges the gap between technical interview requirements and project planning, eliminating manual ticket creation and ensuring interview specifications are properly tracked in the team's project management workflow. | 6 | 5 | 11 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/providers | The `providers` module serves as an abstraction layer for integrating AI language models into the application, allowing the system to work with different AI providers (like Anthropic's Claude) through a unified interface. It handles critical business operations including token usage tracking, cost calculation based on API pricing, and role-based model assignment (e.g., using cheaper models for simple tasks like summarization and more powerful models for complex tasks like code review). The module also supports both standard API-based access and Claude Max subscription integration, enabling flexible cost management and AI resource allocation across different agent roles. | 6 | 1 | 6 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/review | The `review` module orchestrates a two-pass automated code review process: first running validation checks, then conducting an AI-powered deep review with strict false-positive prevention rules. It transforms code review from a manual handoff into an automated pipeline by packaging each identified issue with specific, actionable instructions that downstream AI agents can immediately use to verify and fix problems. The system prioritizes precision over thoroughness—using a mandatory 5-point self-check and banning speculative language—to ensure that reported issues are genuine bugs or security vulnerabilities rather than style preferences or hypothetical concerns. | 5 | 6 | 7 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/types | The `imp/types` module provides the foundational data structures that standardize how the system tracks and reports AI agent execution across the platform. It defines two core types: `TokenUsage` for measuring and costing API consumption, and `AgentResult` for capturing agent outputs in a consistent format. This module serves as the common vocabulary layer that enables different parts of the system to reliably communicate about agent operations and their resource usage. | 2 | 0 | 2 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/validation | The validation module provides automated code quality assurance for software projects by detecting the project's toolchain (e.g., Python, JavaScript) and running appropriate validation checks like linting, type checking, and testing. It orchestrates multiple validation "gates" that can be executed individually or together, capturing detailed results about what passed or failed. When issues are detected, the module can automatically attempt to fix them using the project's configured auto-formatting and repair tools, streamlining the development workflow and ensuring code meets quality standards before deployment. | 7 | 6 | 8 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests | This module contains the **test suite** for the `imp` command-line application. It ensures quality and reliability by automatically verifying that core commands (init, check, interview, review) are properly implemented and functioning as expected, while also preventing accidental real API calls during testing. The tests act as a safety net to catch bugs and regressions before they reach end users. | 3 | 9 | 0 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/context | Looking at the test files and their contents, I can provide a business-oriented summary:

This test module validates a **codebase intelligence and indexing system** that automatically scans software projects to extract their structure and organization. It ensures the system can accurately discover files, parse code to extract functions/classes/imports, generate navigational index files (`.index.md`), and cache scanning results for performance. The tests verify the system works across multiple programming languages (Python, TypeScript/JavaScript) and follows a rigorous test-driven development approach to achieve 100% branch coverage. | 6 | 46 | 25 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/e2e | I'll explore the e2e module directory to understand its contents and purpose.Based on my analysis of the codebase:

**The `e2e` module is designated for end-to-end testing of the imp framework's complete AI-powered engineering workflows.** This module is intended to validate entire business scenarios from start to finish, such as running a full planning-validation-review-metrics cycle on real-world engineering tasks. Currently unpopulated, it represents a placeholder for comprehensive system tests that will verify the framework's ability to coordinate AI agents through complete workflow scenarios, ensuring all components work together correctly in production-like conditions. | 1 | 0 | 0 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/integration | This integration testing module verifies that the teambuild system's complete workflows function correctly from end to end. It tests the full pipeline from gathering project requirements through interviews, converting those requirements into actionable project management tickets, validating code quality across different toolchains, and performing AI-powered code reviews with automated fixes. The tests ensure that all components—requirements validation, ticket planning, metrics collection, code analysis, and AI review—work together seamlessly to deliver a complete project management and quality assurance solution for development teams. | 9 | 12 | 50 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/interview | This test module validates the **interview specification management system**, which helps teams ensure stakeholder interviews are properly structured and complete before execution. It tests functionality for validating interview specifications against completeness criteria, scoring them based on required components (like stakeholder profiles, objectives, and interview modes), identifying gaps with severity levels (critical, major, minor), and importing validated specs into the system. The module ensures teams can confidently assess whether their interview plans have all necessary elements to gather meaningful stakeholder requirements. | 4 | 29 | 10 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/metrics | This module contains the test suite for the metrics tracking system, which validates that the application can correctly collect, model, and store operational metrics data. It ensures the reliability of the system's ability to capture important events (like token usage and agent activities) and persist them in JSONL format for analysis. These tests safeguard the metrics infrastructure that enables monitoring, analytics, and cost tracking for the application's AI agent operations. | 4 | 0 | 4 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/pm | This module tests the project management (PM) integration functionality that automatically converts structured interview specifications into trackable work tickets in external PM systems like Plane. It validates the entire workflow from parsing specification documents to creating parent and child tickets with proper relationships, priorities, descriptions, and metadata. The tests ensure reliable ticket creation, status updates, filtering, and error handling for teams that want to transform requirements gathering directly into actionable PM tickets. | 6 | 63 | 10 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/providers | This module contains the test suite that validates the AI model provider system's reliability and correctness. It ensures that the application can properly integrate with different AI providers (like Claude SDK and Pydantic AI), accurately track token usage and costs, and correctly manage provider configurations and pricing models. In essence, it safeguards the business-critical functionality of connecting to and managing AI services, preventing costly errors in API integrations and billing calculations. | 7 | 0 | 16 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/review | This test module validates a **code review automation system** that performs quality assurance on code through a two-pass workflow: automated validation checks followed by AI-powered deep analysis. The system identifies code issues with categorized severity levels, generates actionable handoffs with fix recommendations, and supports an automated review-fix-retry workflow with circuit breaker protection. It ensures the review layer can reliably detect problems, track metrics, and guide developers toward resolving quality issues efficiently. | 3 | 0 | 14 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/smoke | Based on the module structure and docstrings, here's a business summary:

**The smoke tests module validates that core system components work correctly in real-world usage scenarios, not just in isolated test environments.** It provides standalone validation scripts for critical modules (context, interview, metrics, project management, provider, review, and validation) that test end-to-end workflows as actual developers would use them—including imports, model construction, serialization, and API interactions. These tests serve as a quick health check to ensure the system's modules function properly in production-like conditions, with simple pass/fail exit codes for rapid validation during development or deployment. | 8 | 79 | 0 |
| /Users/josh/Desktop/MetrIQ/projects/teambuild/imp/tests/validation | Based on the test files and their structure, this module tests a **code quality validation and auto-remediation system**. 

The validation module being tested is responsible for detecting project toolchains (identifying what type of project and build tools are being used), running automated quality gates (validation checks), and automatically fixing detected issues. It provides CLI commands that allow developers to check their projects for problems, receive validation results with pass/fail status, and apply automated fixes to resolve common issues—functioning as a continuous integration quality control system that can both identify and remediate code quality problems. | 7 | 0 | 17 |

## File Types
- Python: 103 files (26,036 lines)

## Key Exports
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/context/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/context/__init__.py`: ClassInfo, DirectoryModule, FileInfo, FunctionInfo, ImportInfo, InvokeFn, Language, ModuleInfo, ProjectScan, StaleModule, SummaryEntry, build_prompt, detect_language, detect_stale_modules, discover_files, generate_indexes, init_command, load_previous_scan, load_summaries, parse_file, parse_python, parse_typescript, render_module_index, render_root_index, save_cache, save_summaries, scan_and_parse, scan_project, summarize_module, summarize_project
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/interview/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/interview/__init__.py`: CompletenessResult, GapSeverity, InterviewMetadata, InterviewMode, InterviewSpec, SpecComponent, SpecGap, StakeholderProfile, get_default_skill_path, get_domain_library_path, list_available_domains, validate_spec, validate_spec_file
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/interview/skills/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/interview/skills/__init__.py`: get_default_skill_path, get_domain_library_path, list_available_domains
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/metrics/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/metrics/__init__.py`: EventType, MetricsCollector, MetricsEvent, MetricsStorage
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/pm/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/pm/__init__.py`: PMAdapter, PMError, PlanResult, PlaneAdapter, PlaneConfig, Ticket, TicketFilter, TicketPriority, TicketRef, TicketSpec, TicketStatus, plan_command, spec_to_tickets
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/providers/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/providers/__init__.py`: AgentProvider, AgentResult, ModelRoster, ProviderConfig, PydanticAIProvider, TokenUsage, calculate_cost
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/providers/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/providers/base.py`: AgentProvider, AgentResult, TokenUsage
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/review/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/review/__init__.py`: ReviewCategory, ReviewHandoff, ReviewIssue, ReviewResult, ReviewRunner, ReviewSeverity
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/types/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/types/__init__.py`: AgentResult, TokenUsage
- `/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/validation/Users/josh/Desktop/MetrIQ/projects/teambuild/imp/src/imp/validation/__init__.py`: FixResult, GateResult, GateRunner, GateType, ProjectType, ToolchainConfig, ValidationResult, ValidationRunner, apply_fix, check_command, detect_toolchain, get_fix_command, run_gate
